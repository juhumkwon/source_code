{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAlPu15mJmkSRUN2UFAfwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/source_code/blob/main/Transformer_%EC%98%88%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "NG1ho8s0doRm",
        "outputId": "61fc266c-b801-4a96-ec76-ef675d2e56cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d013a57f779a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtarget_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Je suis étudiant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bonjour le monde\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msrc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtgt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, LayerNormalization, MultiHeadAttention\n",
        "import numpy as np\n",
        "\n",
        "# **데이터 준비**\n",
        "def tokenize_sentence(sentence, max_length=10):\n",
        "    tokens = [ord(c) for c in sentence.ljust(max_length)]\n",
        "    return np.array(tokens, dtype=np.int32)\n",
        "\n",
        "source_sentences = [\"I am a student\", \"Hello world\"]\n",
        "target_sentences = [\"Je suis étudiant\", \"Bonjour le monde\"]\n",
        "\n",
        "src_data = np.array([tokenize_sentence(s) for s in source_sentences])\n",
        "tgt_data = np.array([tokenize_sentence(s) for s in target_sentences])\n",
        "\n",
        "# **트랜스포머 레이어 정의**\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.attention(x, x)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# **트랜스포머 모델**\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embed_dim=16, num_heads=2, ff_dim=64):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.decoder = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.final_layer = Dense(vocab_size)\n",
        "\n",
        "    def call(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        src = self.encoder(src)\n",
        "\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.decoder(tgt)\n",
        "\n",
        "        return self.final_layer(tgt)\n",
        "\n",
        "# **모델 초기화**\n",
        "vocab_size = 128  # ASCII 문자 개수\n",
        "model = Transformer(vocab_size)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# **모델 학습 (Training)**\n",
        "batch_size = 2\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(src_data, tgt_data[:, :-1])\n",
        "        loss = loss_fn(tgt_data[:, 1:], output)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "# **추론 함수 (Inference)**\n",
        "def translate(model, sentence, max_length=10):\n",
        "    src = tokenize_sentence(sentence, max_length).reshape(1, -1)\n",
        "    src_emb = model.embedding(src)\n",
        "    src_enc = model.encoder(src_emb)\n",
        "\n",
        "    tgt_tokens = [tokenize_sentence(\"<sos>\", max_length)[0]]\n",
        "    for _ in range(max_length):\n",
        "        tgt = np.array(tgt_tokens).reshape(1, -1)\n",
        "        tgt_emb = model.embedding(tgt)\n",
        "        tgt_dec = model.decoder(tgt_emb)\n",
        "        output = model.final_layer(tgt_dec)\n",
        "        next_token = np.argmax(output[:, -1, :])\n",
        "        if next_token == tokenize_sentence(\"<eos>\", max_length)[0]:\n",
        "            break\n",
        "        tgt_tokens.append(next_token)\n",
        "\n",
        "    return \"\".join([chr(i) for i in tgt_tokens])\n",
        "\n",
        "# **번역 실행**\n",
        "sentence = \"I am a student\"\n",
        "translation = translate(model, sentence)\n",
        "print(\"Translated:\", translation)"
      ]
    }
  ]
}