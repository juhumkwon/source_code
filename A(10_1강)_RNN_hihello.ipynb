{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC2FlPfnJhjxlXZUfIjfBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/source_code/blob/main/A(10_1%EA%B0%95)_RNN_hihello.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UusHaeF7BU25"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 준비\n",
        "text = \"hihello\"\n",
        "chars = sorted(set(text))  # 고유한 문자들 (['e', 'h', 'i', 'l', 'o'])\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}  # 문자 -> 인덱스 변환\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}  # 인덱스 -> 문자 변환\n",
        "\n",
        "# 입력과 출력 데이터 생성\n",
        "input_seq = [char_to_idx[char] for char in text[:-1]]  # 'hihell'에 해당하는 인덱스들\n",
        "output_seq = [char_to_idx[char] for char in text[1:]]  # 'ihello'에 해당하는 인덱스들\n",
        "\n",
        "# RNN 학습을 위해 입력 데이터를 (배치 크기, 타임스텝, 특징 수) 형태로 변환\n",
        "input_seq = np.array(input_seq).reshape(1, -1)  # (1, 6)\n",
        "output_seq = np.array(output_seq).reshape(1, -1)  # (1, 6)\n",
        "\n",
        "# 하이퍼파라미터 정의\n",
        "vocab_size = len(chars)  # 고유한 문자 개수\n",
        "embedding_dim = 10  # 임베딩 차원\n",
        "rnn_units = 50  # RNN 유닛 수\n",
        "\n",
        "# 모델 구성\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=input_seq.shape[1]),\n",
        "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(input_seq, output_seq, epochs=500)\n",
        "\n",
        "# 학습된 모델로 다음 문자 예측\n",
        "def predict_next_char(input_char):\n",
        "    input_idx = np.array([char_to_idx[input_char]]).reshape(1, 1)\n",
        "    pred = model.predict(input_idx)\n",
        "    pred_char = idx_to_char[np.argmax(pred)]\n",
        "    return pred_char\n",
        "\n",
        "# 'h'로 시작하는 문자열에 대해 다음 문자 예측\n",
        "start_char = 'h'\n",
        "predicted_char = predict_next_char(start_char)\n",
        "print(f\"Next character after '{start_char}' is: '{predicted_char}'\")"
      ]
    }
  ]
}