{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNY+dBQi6dDz1ijrH1IGMqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/source_code/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYojzcAqfQL5",
        "outputId": "69a8da4e-a69f-463e-8f89-e92e5a266a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['transformer/transformer_block/multi_head_attention/query/kernel', 'transformer/transformer_block/multi_head_attention/query/bias', 'transformer/transformer_block/multi_head_attention/key/kernel', 'transformer/transformer_block/multi_head_attention/key/bias', 'transformer/transformer_block/multi_head_attention/value/kernel', 'transformer/transformer_block/multi_head_attention/value/bias', 'transformer/transformer_block/multi_head_attention/attention_output/kernel', 'transformer/transformer_block/multi_head_attention/attention_output/bias', 'transformer/transformer_block/sequential/dense/kernel', 'transformer/transformer_block/sequential/dense/bias', 'transformer/transformer_block/sequential/dense_1/kernel', 'transformer/transformer_block/sequential/dense_1/bias', 'transformer/transformer_block/layer_normalization/gamma', 'transformer/transformer_block/layer_normalization/beta', 'transformer/transformer_block/layer_normalization_1/gamma', 'transformer/transformer_block/layer_normalization_1/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 4.9523\n",
            "Epoch 2: Loss = 4.8139\n",
            "Epoch 3: Loss = 4.6896\n",
            "Epoch 4: Loss = 4.5804\n",
            "Epoch 5: Loss = 4.4868\n",
            "Epoch 6: Loss = 4.4068\n",
            "Epoch 7: Loss = 4.3384\n",
            "Epoch 8: Loss = 4.2780\n",
            "Epoch 9: Loss = 4.2234\n",
            "Epoch 10: Loss = 4.1737\n",
            "Epoch 11: Loss = 4.1273\n",
            "Epoch 12: Loss = 4.0830\n",
            "Epoch 13: Loss = 4.0405\n",
            "Epoch 14: Loss = 3.9987\n",
            "Epoch 15: Loss = 3.9575\n",
            "Epoch 16: Loss = 3.9167\n",
            "Epoch 17: Loss = 3.8762\n",
            "Epoch 18: Loss = 3.8360\n",
            "Epoch 19: Loss = 3.7961\n",
            "Epoch 20: Loss = 3.7571\n",
            "Epoch 21: Loss = 3.7188\n",
            "Epoch 22: Loss = 3.6813\n",
            "Epoch 23: Loss = 3.6448\n",
            "Epoch 24: Loss = 3.6091\n",
            "Epoch 25: Loss = 3.5740\n",
            "Epoch 26: Loss = 3.5393\n",
            "Epoch 27: Loss = 3.5051\n",
            "Epoch 28: Loss = 3.4713\n",
            "Epoch 29: Loss = 3.4384\n",
            "Epoch 30: Loss = 3.4064\n",
            "Epoch 31: Loss = 3.3753\n",
            "Epoch 32: Loss = 3.3452\n",
            "Epoch 33: Loss = 3.3157\n",
            "Epoch 34: Loss = 3.2866\n",
            "Epoch 35: Loss = 3.2578\n",
            "Epoch 36: Loss = 3.2291\n",
            "Epoch 37: Loss = 3.2005\n",
            "Epoch 38: Loss = 3.1721\n",
            "Epoch 39: Loss = 3.1440\n",
            "Epoch 40: Loss = 3.1163\n",
            "Epoch 41: Loss = 3.0891\n",
            "Epoch 42: Loss = 3.0623\n",
            "Epoch 43: Loss = 3.0359\n",
            "Epoch 44: Loss = 3.0097\n",
            "Epoch 45: Loss = 2.9836\n",
            "Epoch 46: Loss = 2.9577\n",
            "Epoch 47: Loss = 2.9319\n",
            "Epoch 48: Loss = 2.9064\n",
            "Epoch 49: Loss = 2.8812\n",
            "Epoch 50: Loss = 2.8564\n",
            "Epoch 51: Loss = 2.8319\n",
            "Epoch 52: Loss = 2.8075\n",
            "Epoch 53: Loss = 2.7834\n",
            "Epoch 54: Loss = 2.7594\n",
            "Epoch 55: Loss = 2.7357\n",
            "Epoch 56: Loss = 2.7122\n",
            "Epoch 57: Loss = 2.6888\n",
            "Epoch 58: Loss = 2.6656\n",
            "Epoch 59: Loss = 2.6426\n",
            "Epoch 60: Loss = 2.6198\n",
            "Epoch 61: Loss = 2.5973\n",
            "Epoch 62: Loss = 2.5750\n",
            "Epoch 63: Loss = 2.5529\n",
            "Epoch 64: Loss = 2.5309\n",
            "Epoch 65: Loss = 2.5091\n",
            "Epoch 66: Loss = 2.4874\n",
            "Epoch 67: Loss = 2.4660\n",
            "Epoch 68: Loss = 2.4446\n",
            "Epoch 69: Loss = 2.4233\n",
            "Epoch 70: Loss = 2.4022\n",
            "Epoch 71: Loss = 2.3813\n",
            "Epoch 72: Loss = 2.3605\n",
            "Epoch 73: Loss = 2.3398\n",
            "Epoch 74: Loss = 2.3192\n",
            "Epoch 75: Loss = 2.2988\n",
            "Epoch 76: Loss = 2.2784\n",
            "Epoch 77: Loss = 2.2582\n",
            "Epoch 78: Loss = 2.2381\n",
            "Epoch 79: Loss = 2.2182\n",
            "Epoch 80: Loss = 2.1983\n",
            "Epoch 81: Loss = 2.1786\n",
            "Epoch 82: Loss = 2.1589\n",
            "Epoch 83: Loss = 2.1393\n",
            "Epoch 84: Loss = 2.1198\n",
            "Epoch 85: Loss = 2.1003\n",
            "Epoch 86: Loss = 2.0809\n",
            "Epoch 87: Loss = 2.0616\n",
            "Epoch 88: Loss = 2.0424\n",
            "Epoch 89: Loss = 2.0234\n",
            "Epoch 90: Loss = 2.0045\n",
            "Epoch 91: Loss = 1.9858\n",
            "Epoch 92: Loss = 1.9672\n",
            "Epoch 93: Loss = 1.9487\n",
            "Epoch 94: Loss = 1.9302\n",
            "Epoch 95: Loss = 1.9118\n",
            "Epoch 96: Loss = 1.8935\n",
            "Epoch 97: Loss = 1.8753\n",
            "Epoch 98: Loss = 1.8572\n",
            "Epoch 99: Loss = 1.8392\n",
            "Epoch 100: Loss = 1.8214\n",
            "Epoch 101: Loss = 1.8036\n",
            "Epoch 102: Loss = 1.7859\n",
            "Epoch 103: Loss = 1.7683\n",
            "Epoch 104: Loss = 1.7508\n",
            "Epoch 105: Loss = 1.7335\n",
            "Epoch 106: Loss = 1.7162\n",
            "Epoch 107: Loss = 1.6990\n",
            "Epoch 108: Loss = 1.6819\n",
            "Epoch 109: Loss = 1.6649\n",
            "Epoch 110: Loss = 1.6480\n",
            "Epoch 111: Loss = 1.6312\n",
            "Epoch 112: Loss = 1.6146\n",
            "Epoch 113: Loss = 1.5981\n",
            "Epoch 114: Loss = 1.5817\n",
            "Epoch 115: Loss = 1.5655\n",
            "Epoch 116: Loss = 1.5494\n",
            "Epoch 117: Loss = 1.5334\n",
            "Epoch 118: Loss = 1.5175\n",
            "Epoch 119: Loss = 1.5017\n",
            "Epoch 120: Loss = 1.4861\n",
            "Epoch 121: Loss = 1.4705\n",
            "Epoch 122: Loss = 1.4551\n",
            "Epoch 123: Loss = 1.4399\n",
            "Epoch 124: Loss = 1.4248\n",
            "Epoch 125: Loss = 1.4098\n",
            "Epoch 126: Loss = 1.3949\n",
            "Epoch 127: Loss = 1.3802\n",
            "Epoch 128: Loss = 1.3657\n",
            "Epoch 129: Loss = 1.3513\n",
            "Epoch 130: Loss = 1.3370\n",
            "Epoch 131: Loss = 1.3229\n",
            "Epoch 132: Loss = 1.3090\n",
            "Epoch 133: Loss = 1.2952\n",
            "Epoch 134: Loss = 1.2816\n",
            "Epoch 135: Loss = 1.2682\n",
            "Epoch 136: Loss = 1.2549\n",
            "Epoch 137: Loss = 1.2417\n",
            "Epoch 138: Loss = 1.2287\n",
            "Epoch 139: Loss = 1.2159\n",
            "Epoch 140: Loss = 1.2033\n",
            "Epoch 141: Loss = 1.1908\n",
            "Epoch 142: Loss = 1.1784\n",
            "Epoch 143: Loss = 1.1662\n",
            "Epoch 144: Loss = 1.1541\n",
            "Epoch 145: Loss = 1.1422\n",
            "Epoch 146: Loss = 1.1305\n",
            "Epoch 147: Loss = 1.1190\n",
            "Epoch 148: Loss = 1.1076\n",
            "Epoch 149: Loss = 1.0964\n",
            "Epoch 150: Loss = 1.0853\n",
            "Epoch 151: Loss = 1.0744\n",
            "Epoch 152: Loss = 1.0636\n",
            "Epoch 153: Loss = 1.0530\n",
            "Epoch 154: Loss = 1.0426\n",
            "Epoch 155: Loss = 1.0324\n",
            "Epoch 156: Loss = 1.0223\n",
            "Epoch 157: Loss = 1.0124\n",
            "Epoch 158: Loss = 1.0026\n",
            "Epoch 159: Loss = 0.9930\n",
            "Epoch 160: Loss = 0.9836\n",
            "Epoch 161: Loss = 0.9743\n",
            "Epoch 162: Loss = 0.9651\n",
            "Epoch 163: Loss = 0.9562\n",
            "Epoch 164: Loss = 0.9473\n",
            "Epoch 165: Loss = 0.9386\n",
            "Epoch 166: Loss = 0.9301\n",
            "Epoch 167: Loss = 0.9217\n",
            "Epoch 168: Loss = 0.9135\n",
            "Epoch 169: Loss = 0.9054\n",
            "Epoch 170: Loss = 0.8974\n",
            "Epoch 171: Loss = 0.8895\n",
            "Epoch 172: Loss = 0.8818\n",
            "Epoch 173: Loss = 0.8743\n",
            "Epoch 174: Loss = 0.8668\n",
            "Epoch 175: Loss = 0.8595\n",
            "Epoch 176: Loss = 0.8523\n",
            "Epoch 177: Loss = 0.8452\n",
            "Epoch 178: Loss = 0.8382\n",
            "Epoch 179: Loss = 0.8314\n",
            "Epoch 180: Loss = 0.8247\n",
            "Epoch 181: Loss = 0.8180\n",
            "Epoch 182: Loss = 0.8115\n",
            "Epoch 183: Loss = 0.8052\n",
            "Epoch 184: Loss = 0.7989\n",
            "Epoch 185: Loss = 0.7927\n",
            "Epoch 186: Loss = 0.7866\n",
            "Epoch 187: Loss = 0.7807\n",
            "Epoch 188: Loss = 0.7748\n",
            "Epoch 189: Loss = 0.7690\n",
            "Epoch 190: Loss = 0.7633\n",
            "Epoch 191: Loss = 0.7578\n",
            "Epoch 192: Loss = 0.7523\n",
            "Epoch 193: Loss = 0.7469\n",
            "Epoch 194: Loss = 0.7416\n",
            "Epoch 195: Loss = 0.7364\n",
            "Epoch 196: Loss = 0.7313\n",
            "Epoch 197: Loss = 0.7262\n",
            "Epoch 198: Loss = 0.7213\n",
            "Epoch 199: Loss = 0.7164\n",
            "Epoch 200: Loss = 0.7116\n",
            "Epoch 201: Loss = 0.7069\n",
            "Epoch 202: Loss = 0.7023\n",
            "Epoch 203: Loss = 0.6977\n",
            "Epoch 204: Loss = 0.6932\n",
            "Epoch 205: Loss = 0.6888\n",
            "Epoch 206: Loss = 0.6845\n",
            "Epoch 207: Loss = 0.6802\n",
            "Epoch 208: Loss = 0.6761\n",
            "Epoch 209: Loss = 0.6719\n",
            "Epoch 210: Loss = 0.6679\n",
            "Epoch 211: Loss = 0.6639\n",
            "Epoch 212: Loss = 0.6600\n",
            "Epoch 213: Loss = 0.6561\n",
            "Epoch 214: Loss = 0.6524\n",
            "Epoch 215: Loss = 0.6486\n",
            "Epoch 216: Loss = 0.6450\n",
            "Epoch 217: Loss = 0.6414\n",
            "Epoch 218: Loss = 0.6378\n",
            "Epoch 219: Loss = 0.6343\n",
            "Epoch 220: Loss = 0.6309\n",
            "Epoch 221: Loss = 0.6275\n",
            "Epoch 222: Loss = 0.6242\n",
            "Epoch 223: Loss = 0.6209\n",
            "Epoch 224: Loss = 0.6177\n",
            "Epoch 225: Loss = 0.6145\n",
            "Epoch 226: Loss = 0.6114\n",
            "Epoch 227: Loss = 0.6084\n",
            "Epoch 228: Loss = 0.6053\n",
            "Epoch 229: Loss = 0.6024\n",
            "Epoch 230: Loss = 0.5995\n",
            "Epoch 231: Loss = 0.5966\n",
            "Epoch 232: Loss = 0.5938\n",
            "Epoch 233: Loss = 0.5910\n",
            "Epoch 234: Loss = 0.5882\n",
            "Epoch 235: Loss = 0.5855\n",
            "Epoch 236: Loss = 0.5829\n",
            "Epoch 237: Loss = 0.5803\n",
            "Epoch 238: Loss = 0.5777\n",
            "Epoch 239: Loss = 0.5751\n",
            "Epoch 240: Loss = 0.5726\n",
            "Epoch 241: Loss = 0.5702\n",
            "Epoch 242: Loss = 0.5678\n",
            "Epoch 243: Loss = 0.5654\n",
            "Epoch 244: Loss = 0.5630\n",
            "Epoch 245: Loss = 0.5607\n",
            "Epoch 246: Loss = 0.5585\n",
            "Epoch 247: Loss = 0.5562\n",
            "Epoch 248: Loss = 0.5540\n",
            "Epoch 249: Loss = 0.5518\n",
            "Epoch 250: Loss = 0.5497\n",
            "Epoch 251: Loss = 0.5476\n",
            "Epoch 252: Loss = 0.5455\n",
            "Epoch 253: Loss = 0.5434\n",
            "Epoch 254: Loss = 0.5414\n",
            "Epoch 255: Loss = 0.5394\n",
            "Epoch 256: Loss = 0.5375\n",
            "Epoch 257: Loss = 0.5355\n",
            "Epoch 258: Loss = 0.5336\n",
            "Epoch 259: Loss = 0.5318\n",
            "Epoch 260: Loss = 0.5299\n",
            "Epoch 261: Loss = 0.5281\n",
            "Epoch 262: Loss = 0.5263\n",
            "Epoch 263: Loss = 0.5245\n",
            "Epoch 264: Loss = 0.5228\n",
            "Epoch 265: Loss = 0.5211\n",
            "Epoch 266: Loss = 0.5194\n",
            "Epoch 267: Loss = 0.5177\n",
            "Epoch 268: Loss = 0.5160\n",
            "Epoch 269: Loss = 0.5144\n",
            "Epoch 270: Loss = 0.5128\n",
            "Epoch 271: Loss = 0.5112\n",
            "Epoch 272: Loss = 0.5097\n",
            "Epoch 273: Loss = 0.5081\n",
            "Epoch 274: Loss = 0.5066\n",
            "Epoch 275: Loss = 0.5051\n",
            "Epoch 276: Loss = 0.5036\n",
            "Epoch 277: Loss = 0.5022\n",
            "Epoch 278: Loss = 0.5007\n",
            "Epoch 279: Loss = 0.4993\n",
            "Epoch 280: Loss = 0.4979\n",
            "Epoch 281: Loss = 0.4966\n",
            "Epoch 282: Loss = 0.4952\n",
            "Epoch 283: Loss = 0.4938\n",
            "Epoch 284: Loss = 0.4925\n",
            "Epoch 285: Loss = 0.4912\n",
            "Epoch 286: Loss = 0.4899\n",
            "Epoch 287: Loss = 0.4886\n",
            "Epoch 288: Loss = 0.4874\n",
            "Epoch 289: Loss = 0.4861\n",
            "Epoch 290: Loss = 0.4849\n",
            "Epoch 291: Loss = 0.4837\n",
            "Epoch 292: Loss = 0.4825\n",
            "Epoch 293: Loss = 0.4813\n",
            "Epoch 294: Loss = 0.4802\n",
            "Epoch 295: Loss = 0.4790\n",
            "Epoch 296: Loss = 0.4779\n",
            "Epoch 297: Loss = 0.4768\n",
            "Epoch 298: Loss = 0.4757\n",
            "Epoch 299: Loss = 0.4746\n",
            "Epoch 300: Loss = 0.4735\n",
            "Epoch 301: Loss = 0.4724\n",
            "Epoch 302: Loss = 0.4714\n",
            "Epoch 303: Loss = 0.4703\n",
            "Epoch 304: Loss = 0.4693\n",
            "Epoch 305: Loss = 0.4683\n",
            "Epoch 306: Loss = 0.4673\n",
            "Epoch 307: Loss = 0.4663\n",
            "Epoch 308: Loss = 0.4653\n",
            "Epoch 309: Loss = 0.4643\n",
            "Epoch 310: Loss = 0.4634\n",
            "Epoch 311: Loss = 0.4624\n",
            "Epoch 312: Loss = 0.4615\n",
            "Epoch 313: Loss = 0.4606\n",
            "Epoch 314: Loss = 0.4597\n",
            "Epoch 315: Loss = 0.4588\n",
            "Epoch 316: Loss = 0.4579\n",
            "Epoch 317: Loss = 0.4570\n",
            "Epoch 318: Loss = 0.4561\n",
            "Epoch 319: Loss = 0.4553\n",
            "Epoch 320: Loss = 0.4544\n",
            "Epoch 321: Loss = 0.4536\n",
            "Epoch 322: Loss = 0.4527\n",
            "Epoch 323: Loss = 0.4519\n",
            "Epoch 324: Loss = 0.4511\n",
            "Epoch 325: Loss = 0.4503\n",
            "Epoch 326: Loss = 0.4495\n",
            "Epoch 327: Loss = 0.4487\n",
            "Epoch 328: Loss = 0.4479\n",
            "Epoch 329: Loss = 0.4472\n",
            "Epoch 330: Loss = 0.4464\n",
            "Epoch 331: Loss = 0.4457\n",
            "Epoch 332: Loss = 0.4449\n",
            "Epoch 333: Loss = 0.4442\n",
            "Epoch 334: Loss = 0.4435\n",
            "Epoch 335: Loss = 0.4427\n",
            "Epoch 336: Loss = 0.4420\n",
            "Epoch 337: Loss = 0.4413\n",
            "Epoch 338: Loss = 0.4406\n",
            "Epoch 339: Loss = 0.4399\n",
            "Epoch 340: Loss = 0.4393\n",
            "Epoch 341: Loss = 0.4386\n",
            "Epoch 342: Loss = 0.4379\n",
            "Epoch 343: Loss = 0.4373\n",
            "Epoch 344: Loss = 0.4366\n",
            "Epoch 345: Loss = 0.4360\n",
            "Epoch 346: Loss = 0.4353\n",
            "Epoch 347: Loss = 0.4347\n",
            "Epoch 348: Loss = 0.4341\n",
            "Epoch 349: Loss = 0.4334\n",
            "Epoch 350: Loss = 0.4328\n",
            "Epoch 351: Loss = 0.4322\n",
            "Epoch 352: Loss = 0.4316\n",
            "Epoch 353: Loss = 0.4310\n",
            "Epoch 354: Loss = 0.4304\n",
            "Epoch 355: Loss = 0.4299\n",
            "Epoch 356: Loss = 0.4293\n",
            "Epoch 357: Loss = 0.4287\n",
            "Epoch 358: Loss = 0.4281\n",
            "Epoch 359: Loss = 0.4276\n",
            "Epoch 360: Loss = 0.4270\n",
            "Epoch 361: Loss = 0.4265\n",
            "Epoch 362: Loss = 0.4259\n",
            "Epoch 363: Loss = 0.4254\n",
            "Epoch 364: Loss = 0.4249\n",
            "Epoch 365: Loss = 0.4243\n",
            "Epoch 366: Loss = 0.4238\n",
            "Epoch 367: Loss = 0.4233\n",
            "Epoch 368: Loss = 0.4228\n",
            "Epoch 369: Loss = 0.4223\n",
            "Epoch 370: Loss = 0.4218\n",
            "Epoch 371: Loss = 0.4213\n",
            "Epoch 372: Loss = 0.4208\n",
            "Epoch 373: Loss = 0.4203\n",
            "Epoch 374: Loss = 0.4198\n",
            "Epoch 375: Loss = 0.4193\n",
            "Epoch 376: Loss = 0.4189\n",
            "Epoch 377: Loss = 0.4184\n",
            "Epoch 378: Loss = 0.4179\n",
            "Epoch 379: Loss = 0.4175\n",
            "Epoch 380: Loss = 0.4170\n",
            "Epoch 381: Loss = 0.4166\n",
            "Epoch 382: Loss = 0.4161\n",
            "Epoch 383: Loss = 0.4157\n",
            "Epoch 384: Loss = 0.4152\n",
            "Epoch 385: Loss = 0.4148\n",
            "Epoch 386: Loss = 0.4144\n",
            "Epoch 387: Loss = 0.4139\n",
            "Epoch 388: Loss = 0.4135\n",
            "Epoch 389: Loss = 0.4131\n",
            "Epoch 390: Loss = 0.4127\n",
            "Epoch 391: Loss = 0.4123\n",
            "Epoch 392: Loss = 0.4119\n",
            "Epoch 393: Loss = 0.4114\n",
            "Epoch 394: Loss = 0.4110\n",
            "Epoch 395: Loss = 0.4106\n",
            "Epoch 396: Loss = 0.4103\n",
            "Epoch 397: Loss = 0.4099\n",
            "Epoch 398: Loss = 0.4095\n",
            "Epoch 399: Loss = 0.4091\n",
            "Epoch 400: Loss = 0.4087\n",
            "Epoch 401: Loss = 0.4083\n",
            "Epoch 402: Loss = 0.4080\n",
            "Epoch 403: Loss = 0.4076\n",
            "Epoch 404: Loss = 0.4072\n",
            "Epoch 405: Loss = 0.4069\n",
            "Epoch 406: Loss = 0.4065\n",
            "Epoch 407: Loss = 0.4061\n",
            "Epoch 408: Loss = 0.4058\n",
            "Epoch 409: Loss = 0.4054\n",
            "Epoch 410: Loss = 0.4051\n",
            "Epoch 411: Loss = 0.4047\n",
            "Epoch 412: Loss = 0.4044\n",
            "Epoch 413: Loss = 0.4040\n",
            "Epoch 414: Loss = 0.4037\n",
            "Epoch 415: Loss = 0.4034\n",
            "Epoch 416: Loss = 0.4030\n",
            "Epoch 417: Loss = 0.4027\n",
            "Epoch 418: Loss = 0.4024\n",
            "Epoch 419: Loss = 0.4021\n",
            "Epoch 420: Loss = 0.4017\n",
            "Epoch 421: Loss = 0.4014\n",
            "Epoch 422: Loss = 0.4011\n",
            "Epoch 423: Loss = 0.4008\n",
            "Epoch 424: Loss = 0.4005\n",
            "Epoch 425: Loss = 0.4002\n",
            "Epoch 426: Loss = 0.3999\n",
            "Epoch 427: Loss = 0.3996\n",
            "Epoch 428: Loss = 0.3992\n",
            "Epoch 429: Loss = 0.3989\n",
            "Epoch 430: Loss = 0.3987\n",
            "Epoch 431: Loss = 0.3984\n",
            "Epoch 432: Loss = 0.3981\n",
            "Epoch 433: Loss = 0.3978\n",
            "Epoch 434: Loss = 0.3975\n",
            "Epoch 435: Loss = 0.3972\n",
            "Epoch 436: Loss = 0.3969\n",
            "Epoch 437: Loss = 0.3966\n",
            "Epoch 438: Loss = 0.3964\n",
            "Epoch 439: Loss = 0.3961\n",
            "Epoch 440: Loss = 0.3958\n",
            "Epoch 441: Loss = 0.3955\n",
            "Epoch 442: Loss = 0.3953\n",
            "Epoch 443: Loss = 0.3950\n",
            "Epoch 444: Loss = 0.3947\n",
            "Epoch 445: Loss = 0.3945\n",
            "Epoch 446: Loss = 0.3942\n",
            "Epoch 447: Loss = 0.3940\n",
            "Epoch 448: Loss = 0.3937\n",
            "Epoch 449: Loss = 0.3934\n",
            "Epoch 450: Loss = 0.3932\n",
            "Epoch 451: Loss = 0.3929\n",
            "Epoch 452: Loss = 0.3927\n",
            "Epoch 453: Loss = 0.3924\n",
            "Epoch 454: Loss = 0.3922\n",
            "Epoch 455: Loss = 0.3919\n",
            "Epoch 456: Loss = 0.3917\n",
            "Epoch 457: Loss = 0.3915\n",
            "Epoch 458: Loss = 0.3912\n",
            "Epoch 459: Loss = 0.3910\n",
            "Epoch 460: Loss = 0.3907\n",
            "Epoch 461: Loss = 0.3905\n",
            "Epoch 462: Loss = 0.3903\n",
            "Epoch 463: Loss = 0.3900\n",
            "Epoch 464: Loss = 0.3898\n",
            "Epoch 465: Loss = 0.3896\n",
            "Epoch 466: Loss = 0.3894\n",
            "Epoch 467: Loss = 0.3891\n",
            "Epoch 468: Loss = 0.3889\n",
            "Epoch 469: Loss = 0.3887\n",
            "Epoch 470: Loss = 0.3885\n",
            "Epoch 471: Loss = 0.3883\n",
            "Epoch 472: Loss = 0.3880\n",
            "Epoch 473: Loss = 0.3878\n",
            "Epoch 474: Loss = 0.3876\n",
            "Epoch 475: Loss = 0.3874\n",
            "Epoch 476: Loss = 0.3872\n",
            "Epoch 477: Loss = 0.3870\n",
            "Epoch 478: Loss = 0.3868\n",
            "Epoch 479: Loss = 0.3866\n",
            "Epoch 480: Loss = 0.3864\n",
            "Epoch 481: Loss = 0.3862\n",
            "Epoch 482: Loss = 0.3860\n",
            "Epoch 483: Loss = 0.3858\n",
            "Epoch 484: Loss = 0.3856\n",
            "Epoch 485: Loss = 0.3854\n",
            "Epoch 486: Loss = 0.3852\n",
            "Epoch 487: Loss = 0.3850\n",
            "Epoch 488: Loss = 0.3848\n",
            "Epoch 489: Loss = 0.3846\n",
            "Epoch 490: Loss = 0.3844\n",
            "Epoch 491: Loss = 0.3842\n",
            "Epoch 492: Loss = 0.3840\n",
            "Epoch 493: Loss = 0.3838\n",
            "Epoch 494: Loss = 0.3837\n",
            "Epoch 495: Loss = 0.3835\n",
            "Epoch 496: Loss = 0.3833\n",
            "Epoch 497: Loss = 0.3831\n",
            "Epoch 498: Loss = 0.3829\n",
            "Epoch 499: Loss = 0.3827\n",
            "Epoch 500: Loss = 0.3826\n",
            "Epoch 501: Loss = 0.3824\n",
            "Epoch 502: Loss = 0.3822\n",
            "Epoch 503: Loss = 0.3820\n",
            "Epoch 504: Loss = 0.3819\n",
            "Epoch 505: Loss = 0.3817\n",
            "Epoch 506: Loss = 0.3815\n",
            "Epoch 507: Loss = 0.3813\n",
            "Epoch 508: Loss = 0.3812\n",
            "Epoch 509: Loss = 0.3810\n",
            "Epoch 510: Loss = 0.3808\n",
            "Epoch 511: Loss = 0.3807\n",
            "Epoch 512: Loss = 0.3805\n",
            "Epoch 513: Loss = 0.3804\n",
            "Epoch 514: Loss = 0.3802\n",
            "Epoch 515: Loss = 0.3800\n",
            "Epoch 516: Loss = 0.3799\n",
            "Epoch 517: Loss = 0.3797\n",
            "Epoch 518: Loss = 0.3795\n",
            "Epoch 519: Loss = 0.3794\n",
            "Epoch 520: Loss = 0.3792\n",
            "Epoch 521: Loss = 0.3791\n",
            "Epoch 522: Loss = 0.3789\n",
            "Epoch 523: Loss = 0.3788\n",
            "Epoch 524: Loss = 0.3786\n",
            "Epoch 525: Loss = 0.3785\n",
            "Epoch 526: Loss = 0.3783\n",
            "Epoch 527: Loss = 0.3782\n",
            "Epoch 528: Loss = 0.3780\n",
            "Epoch 529: Loss = 0.3779\n",
            "Epoch 530: Loss = 0.3777\n",
            "Epoch 531: Loss = 0.3776\n",
            "Epoch 532: Loss = 0.3774\n",
            "Epoch 533: Loss = 0.3773\n",
            "Epoch 534: Loss = 0.3771\n",
            "Epoch 535: Loss = 0.3770\n",
            "Epoch 536: Loss = 0.3769\n",
            "Epoch 537: Loss = 0.3767\n",
            "Epoch 538: Loss = 0.3766\n",
            "Epoch 539: Loss = 0.3764\n",
            "Epoch 540: Loss = 0.3763\n",
            "Epoch 541: Loss = 0.3762\n",
            "Epoch 542: Loss = 0.3760\n",
            "Epoch 543: Loss = 0.3759\n",
            "Epoch 544: Loss = 0.3758\n",
            "Epoch 545: Loss = 0.3756\n",
            "Epoch 546: Loss = 0.3755\n",
            "Epoch 547: Loss = 0.3754\n",
            "Epoch 548: Loss = 0.3752\n",
            "Epoch 549: Loss = 0.3751\n",
            "Epoch 550: Loss = 0.3750\n",
            "Epoch 551: Loss = 0.3748\n",
            "Epoch 552: Loss = 0.3747\n",
            "Epoch 553: Loss = 0.3746\n",
            "Epoch 554: Loss = 0.3744\n",
            "Epoch 555: Loss = 0.3743\n",
            "Epoch 556: Loss = 0.3742\n",
            "Epoch 557: Loss = 0.3741\n",
            "Epoch 558: Loss = 0.3739\n",
            "Epoch 559: Loss = 0.3738\n",
            "Epoch 560: Loss = 0.3737\n",
            "Epoch 561: Loss = 0.3736\n",
            "Epoch 562: Loss = 0.3735\n",
            "Epoch 563: Loss = 0.3733\n",
            "Epoch 564: Loss = 0.3732\n",
            "Epoch 565: Loss = 0.3731\n",
            "Epoch 566: Loss = 0.3730\n",
            "Epoch 567: Loss = 0.3729\n",
            "Epoch 568: Loss = 0.3727\n",
            "Epoch 569: Loss = 0.3726\n",
            "Epoch 570: Loss = 0.3725\n",
            "Epoch 571: Loss = 0.3724\n",
            "Epoch 572: Loss = 0.3723\n",
            "Epoch 573: Loss = 0.3722\n",
            "Epoch 574: Loss = 0.3720\n",
            "Epoch 575: Loss = 0.3719\n",
            "Epoch 576: Loss = 0.3718\n",
            "Epoch 577: Loss = 0.3717\n",
            "Epoch 578: Loss = 0.3716\n",
            "Epoch 579: Loss = 0.3715\n",
            "Epoch 580: Loss = 0.3714\n",
            "Epoch 581: Loss = 0.3713\n",
            "Epoch 582: Loss = 0.3712\n",
            "Epoch 583: Loss = 0.3711\n",
            "Epoch 584: Loss = 0.3709\n",
            "Epoch 585: Loss = 0.3708\n",
            "Epoch 586: Loss = 0.3707\n",
            "Epoch 587: Loss = 0.3706\n",
            "Epoch 588: Loss = 0.3705\n",
            "Epoch 589: Loss = 0.3704\n",
            "Epoch 590: Loss = 0.3703\n",
            "Epoch 591: Loss = 0.3702\n",
            "Epoch 592: Loss = 0.3701\n",
            "Epoch 593: Loss = 0.3700\n",
            "Epoch 594: Loss = 0.3699\n",
            "Epoch 595: Loss = 0.3698\n",
            "Epoch 596: Loss = 0.3697\n",
            "Epoch 597: Loss = 0.3696\n",
            "Epoch 598: Loss = 0.3695\n",
            "Epoch 599: Loss = 0.3694\n",
            "Epoch 600: Loss = 0.3693\n",
            "Epoch 601: Loss = 0.3692\n",
            "Epoch 602: Loss = 0.3691\n",
            "Epoch 603: Loss = 0.3690\n",
            "Epoch 604: Loss = 0.3689\n",
            "Epoch 605: Loss = 0.3688\n",
            "Epoch 606: Loss = 0.3687\n",
            "Epoch 607: Loss = 0.3686\n",
            "Epoch 608: Loss = 0.3685\n",
            "Epoch 609: Loss = 0.3684\n",
            "Epoch 610: Loss = 0.3684\n",
            "Epoch 611: Loss = 0.3683\n",
            "Epoch 612: Loss = 0.3682\n",
            "Epoch 613: Loss = 0.3681\n",
            "Epoch 614: Loss = 0.3680\n",
            "Epoch 615: Loss = 0.3679\n",
            "Epoch 616: Loss = 0.3678\n",
            "Epoch 617: Loss = 0.3677\n",
            "Epoch 618: Loss = 0.3676\n",
            "Epoch 619: Loss = 0.3676\n",
            "Epoch 620: Loss = 0.3675\n",
            "Epoch 621: Loss = 0.3674\n",
            "Epoch 622: Loss = 0.3673\n",
            "Epoch 623: Loss = 0.3673\n",
            "Epoch 624: Loss = 0.3672\n",
            "Epoch 625: Loss = 0.3671\n",
            "Epoch 626: Loss = 0.3670\n",
            "Epoch 627: Loss = 0.3669\n",
            "Epoch 628: Loss = 0.3668\n",
            "Epoch 629: Loss = 0.3667\n",
            "Epoch 630: Loss = 0.3666\n",
            "Epoch 631: Loss = 0.3665\n",
            "Epoch 632: Loss = 0.3665\n",
            "Epoch 633: Loss = 0.3664\n",
            "Epoch 634: Loss = 0.3663\n",
            "Epoch 635: Loss = 0.3662\n",
            "Epoch 636: Loss = 0.3661\n",
            "Epoch 637: Loss = 0.3660\n",
            "Epoch 638: Loss = 0.3659\n",
            "Epoch 639: Loss = 0.3659\n",
            "Epoch 640: Loss = 0.3658\n",
            "Epoch 641: Loss = 0.3657\n",
            "Epoch 642: Loss = 0.3656\n",
            "Epoch 643: Loss = 0.3655\n",
            "Epoch 644: Loss = 0.3655\n",
            "Epoch 645: Loss = 0.3654\n",
            "Epoch 646: Loss = 0.3653\n",
            "Epoch 647: Loss = 0.3652\n",
            "Epoch 648: Loss = 0.3651\n",
            "Epoch 649: Loss = 0.3651\n",
            "Epoch 650: Loss = 0.3650\n",
            "Epoch 651: Loss = 0.3649\n",
            "Epoch 652: Loss = 0.3648\n",
            "Epoch 653: Loss = 0.3648\n",
            "Epoch 654: Loss = 0.3647\n",
            "Epoch 655: Loss = 0.3646\n",
            "Epoch 656: Loss = 0.3645\n",
            "Epoch 657: Loss = 0.3645\n",
            "Epoch 658: Loss = 0.3644\n",
            "Epoch 659: Loss = 0.3643\n",
            "Epoch 660: Loss = 0.3642\n",
            "Epoch 661: Loss = 0.3642\n",
            "Epoch 662: Loss = 0.3641\n",
            "Epoch 663: Loss = 0.3640\n",
            "Epoch 664: Loss = 0.3640\n",
            "Epoch 665: Loss = 0.3639\n",
            "Epoch 666: Loss = 0.3638\n",
            "Epoch 667: Loss = 0.3638\n",
            "Epoch 668: Loss = 0.3637\n",
            "Epoch 669: Loss = 0.3636\n",
            "Epoch 670: Loss = 0.3635\n",
            "Epoch 671: Loss = 0.3635\n",
            "Epoch 672: Loss = 0.3634\n",
            "Epoch 673: Loss = 0.3633\n",
            "Epoch 674: Loss = 0.3633\n",
            "Epoch 675: Loss = 0.3632\n",
            "Epoch 676: Loss = 0.3631\n",
            "Epoch 677: Loss = 0.3631\n",
            "Epoch 678: Loss = 0.3630\n",
            "Epoch 679: Loss = 0.3629\n",
            "Epoch 680: Loss = 0.3629\n",
            "Epoch 681: Loss = 0.3628\n",
            "Epoch 682: Loss = 0.3627\n",
            "Epoch 683: Loss = 0.3627\n",
            "Epoch 684: Loss = 0.3626\n",
            "Epoch 685: Loss = 0.3626\n",
            "Epoch 686: Loss = 0.3625\n",
            "Epoch 687: Loss = 0.3625\n",
            "Epoch 688: Loss = 0.3625\n",
            "Epoch 689: Loss = 0.3625\n",
            "Epoch 690: Loss = 0.3625\n",
            "Epoch 691: Loss = 0.3623\n",
            "Epoch 692: Loss = 0.3621\n",
            "Epoch 693: Loss = 0.3620\n",
            "Epoch 694: Loss = 0.3620\n",
            "Epoch 695: Loss = 0.3620\n",
            "Epoch 696: Loss = 0.3619\n",
            "Epoch 697: Loss = 0.3618\n",
            "Epoch 698: Loss = 0.3617\n",
            "Epoch 699: Loss = 0.3617\n",
            "Epoch 700: Loss = 0.3617\n",
            "Epoch 701: Loss = 0.3616\n",
            "Epoch 702: Loss = 0.3615\n",
            "Epoch 703: Loss = 0.3614\n",
            "Epoch 704: Loss = 0.3614\n",
            "Epoch 705: Loss = 0.3613\n",
            "Epoch 706: Loss = 0.3612\n",
            "Epoch 707: Loss = 0.3612\n",
            "Epoch 708: Loss = 0.3612\n",
            "Epoch 709: Loss = 0.3611\n",
            "Epoch 710: Loss = 0.3610\n",
            "Epoch 711: Loss = 0.3610\n",
            "Epoch 712: Loss = 0.3609\n",
            "Epoch 713: Loss = 0.3609\n",
            "Epoch 714: Loss = 0.3608\n",
            "Epoch 715: Loss = 0.3607\n",
            "Epoch 716: Loss = 0.3607\n",
            "Epoch 717: Loss = 0.3606\n",
            "Epoch 718: Loss = 0.3606\n",
            "Epoch 719: Loss = 0.3605\n",
            "Epoch 720: Loss = 0.3605\n",
            "Epoch 721: Loss = 0.3604\n",
            "Epoch 722: Loss = 0.3603\n",
            "Epoch 723: Loss = 0.3603\n",
            "Epoch 724: Loss = 0.3602\n",
            "Epoch 725: Loss = 0.3602\n",
            "Epoch 726: Loss = 0.3601\n",
            "Epoch 727: Loss = 0.3601\n",
            "Epoch 728: Loss = 0.3600\n",
            "Epoch 729: Loss = 0.3600\n",
            "Epoch 730: Loss = 0.3599\n",
            "Epoch 731: Loss = 0.3599\n",
            "Epoch 732: Loss = 0.3598\n",
            "Epoch 733: Loss = 0.3598\n",
            "Epoch 734: Loss = 0.3597\n",
            "Epoch 735: Loss = 0.3596\n",
            "Epoch 736: Loss = 0.3596\n",
            "Epoch 737: Loss = 0.3595\n",
            "Epoch 738: Loss = 0.3595\n",
            "Epoch 739: Loss = 0.3594\n",
            "Epoch 740: Loss = 0.3594\n",
            "Epoch 741: Loss = 0.3593\n",
            "Epoch 742: Loss = 0.3593\n",
            "Epoch 743: Loss = 0.3592\n",
            "Epoch 744: Loss = 0.3592\n",
            "Epoch 745: Loss = 0.3591\n",
            "Epoch 746: Loss = 0.3591\n",
            "Epoch 747: Loss = 0.3590\n",
            "Epoch 748: Loss = 0.3590\n",
            "Epoch 749: Loss = 0.3589\n",
            "Epoch 750: Loss = 0.3589\n",
            "Epoch 751: Loss = 0.3588\n",
            "Epoch 752: Loss = 0.3588\n",
            "Epoch 753: Loss = 0.3588\n",
            "Epoch 754: Loss = 0.3587\n",
            "Epoch 755: Loss = 0.3587\n",
            "Epoch 756: Loss = 0.3586\n",
            "Epoch 757: Loss = 0.3586\n",
            "Epoch 758: Loss = 0.3585\n",
            "Epoch 759: Loss = 0.3585\n",
            "Epoch 760: Loss = 0.3584\n",
            "Epoch 761: Loss = 0.3584\n",
            "Epoch 762: Loss = 0.3583\n",
            "Epoch 763: Loss = 0.3583\n",
            "Epoch 764: Loss = 0.3582\n",
            "Epoch 765: Loss = 0.3582\n",
            "Epoch 766: Loss = 0.3581\n",
            "Epoch 767: Loss = 0.3581\n",
            "Epoch 768: Loss = 0.3581\n",
            "Epoch 769: Loss = 0.3580\n",
            "Epoch 770: Loss = 0.3580\n",
            "Epoch 771: Loss = 0.3579\n",
            "Epoch 772: Loss = 0.3579\n",
            "Epoch 773: Loss = 0.3578\n",
            "Epoch 774: Loss = 0.3578\n",
            "Epoch 775: Loss = 0.3577\n",
            "Epoch 776: Loss = 0.3577\n",
            "Epoch 777: Loss = 0.3577\n",
            "Epoch 778: Loss = 0.3576\n",
            "Epoch 779: Loss = 0.3576\n",
            "Epoch 780: Loss = 0.3575\n",
            "Epoch 781: Loss = 0.3575\n",
            "Epoch 782: Loss = 0.3574\n",
            "Epoch 783: Loss = 0.3574\n",
            "Epoch 784: Loss = 0.3574\n",
            "Epoch 785: Loss = 0.3573\n",
            "Epoch 786: Loss = 0.3573\n",
            "Epoch 787: Loss = 0.3572\n",
            "Epoch 788: Loss = 0.3572\n",
            "Epoch 789: Loss = 0.3572\n",
            "Epoch 790: Loss = 0.3571\n",
            "Epoch 791: Loss = 0.3571\n",
            "Epoch 792: Loss = 0.3571\n",
            "Epoch 793: Loss = 0.3571\n",
            "Epoch 794: Loss = 0.3571\n",
            "Epoch 795: Loss = 0.3571\n",
            "Epoch 796: Loss = 0.3572\n",
            "Epoch 797: Loss = 0.3572\n",
            "Epoch 798: Loss = 0.3570\n",
            "Epoch 799: Loss = 0.3568\n",
            "Epoch 800: Loss = 0.3567\n",
            "Epoch 801: Loss = 0.3567\n",
            "Epoch 802: Loss = 0.3568\n",
            "Epoch 803: Loss = 0.3567\n",
            "Epoch 804: Loss = 0.3566\n",
            "Epoch 805: Loss = 0.3565\n",
            "Epoch 806: Loss = 0.3565\n",
            "Epoch 807: Loss = 0.3565\n",
            "Epoch 808: Loss = 0.3564\n",
            "Epoch 809: Loss = 0.3564\n",
            "Epoch 810: Loss = 0.3563\n",
            "Epoch 811: Loss = 0.3563\n",
            "Epoch 812: Loss = 0.3563\n",
            "Epoch 813: Loss = 0.3562\n",
            "Epoch 814: Loss = 0.3562\n",
            "Epoch 815: Loss = 0.3561\n",
            "Epoch 816: Loss = 0.3561\n",
            "Epoch 817: Loss = 0.3561\n",
            "Epoch 818: Loss = 0.3560\n",
            "Epoch 819: Loss = 0.3560\n",
            "Epoch 820: Loss = 0.3560\n",
            "Epoch 821: Loss = 0.3559\n",
            "Epoch 822: Loss = 0.3559\n",
            "Epoch 823: Loss = 0.3558\n",
            "Epoch 824: Loss = 0.3558\n",
            "Epoch 825: Loss = 0.3558\n",
            "Epoch 826: Loss = 0.3557\n",
            "Epoch 827: Loss = 0.3557\n",
            "Epoch 828: Loss = 0.3556\n",
            "Epoch 829: Loss = 0.3556\n",
            "Epoch 830: Loss = 0.3556\n",
            "Epoch 831: Loss = 0.3555\n",
            "Epoch 832: Loss = 0.3555\n",
            "Epoch 833: Loss = 0.3555\n",
            "Epoch 834: Loss = 0.3554\n",
            "Epoch 835: Loss = 0.3554\n",
            "Epoch 836: Loss = 0.3554\n",
            "Epoch 837: Loss = 0.3553\n",
            "Epoch 838: Loss = 0.3553\n",
            "Epoch 839: Loss = 0.3553\n",
            "Epoch 840: Loss = 0.3552\n",
            "Epoch 841: Loss = 0.3552\n",
            "Epoch 842: Loss = 0.3551\n",
            "Epoch 843: Loss = 0.3551\n",
            "Epoch 844: Loss = 0.3551\n",
            "Epoch 845: Loss = 0.3550\n",
            "Epoch 846: Loss = 0.3550\n",
            "Epoch 847: Loss = 0.3550\n",
            "Epoch 848: Loss = 0.3549\n",
            "Epoch 849: Loss = 0.3549\n",
            "Epoch 850: Loss = 0.3549\n",
            "Epoch 851: Loss = 0.3548\n",
            "Epoch 852: Loss = 0.3548\n",
            "Epoch 853: Loss = 0.3548\n",
            "Epoch 854: Loss = 0.3547\n",
            "Epoch 855: Loss = 0.3547\n",
            "Epoch 856: Loss = 0.3547\n",
            "Epoch 857: Loss = 0.3547\n",
            "Epoch 858: Loss = 0.3546\n",
            "Epoch 859: Loss = 0.3546\n",
            "Epoch 860: Loss = 0.3546\n",
            "Epoch 861: Loss = 0.3545\n",
            "Epoch 862: Loss = 0.3545\n",
            "Epoch 863: Loss = 0.3545\n",
            "Epoch 864: Loss = 0.3544\n",
            "Epoch 865: Loss = 0.3544\n",
            "Epoch 866: Loss = 0.3544\n",
            "Epoch 867: Loss = 0.3543\n",
            "Epoch 868: Loss = 0.3543\n",
            "Epoch 869: Loss = 0.3543\n",
            "Epoch 870: Loss = 0.3542\n",
            "Epoch 871: Loss = 0.3542\n",
            "Epoch 872: Loss = 0.3542\n",
            "Epoch 873: Loss = 0.3541\n",
            "Epoch 874: Loss = 0.3541\n",
            "Epoch 875: Loss = 0.3541\n",
            "Epoch 876: Loss = 0.3541\n",
            "Epoch 877: Loss = 0.3540\n",
            "Epoch 878: Loss = 0.3540\n",
            "Epoch 879: Loss = 0.3540\n",
            "Epoch 880: Loss = 0.3539\n",
            "Epoch 881: Loss = 0.3539\n",
            "Epoch 882: Loss = 0.3539\n",
            "Epoch 883: Loss = 0.3538\n",
            "Epoch 884: Loss = 0.3538\n",
            "Epoch 885: Loss = 0.3538\n",
            "Epoch 886: Loss = 0.3538\n",
            "Epoch 887: Loss = 0.3537\n",
            "Epoch 888: Loss = 0.3537\n",
            "Epoch 889: Loss = 0.3537\n",
            "Epoch 890: Loss = 0.3536\n",
            "Epoch 891: Loss = 0.3536\n",
            "Epoch 892: Loss = 0.3536\n",
            "Epoch 893: Loss = 0.3536\n",
            "Epoch 894: Loss = 0.3535\n",
            "Epoch 895: Loss = 0.3535\n",
            "Epoch 896: Loss = 0.3535\n",
            "Epoch 897: Loss = 0.3534\n",
            "Epoch 898: Loss = 0.3534\n",
            "Epoch 899: Loss = 0.3534\n",
            "Epoch 900: Loss = 0.3534\n",
            "Epoch 901: Loss = 0.3533\n",
            "Epoch 902: Loss = 0.3533\n",
            "Epoch 903: Loss = 0.3533\n",
            "Epoch 904: Loss = 0.3533\n",
            "Epoch 905: Loss = 0.3533\n",
            "Epoch 906: Loss = 0.3533\n",
            "Epoch 907: Loss = 0.3534\n",
            "Epoch 908: Loss = 0.3534\n",
            "Epoch 909: Loss = 0.3535\n",
            "Epoch 910: Loss = 0.3534\n",
            "Epoch 911: Loss = 0.3533\n",
            "Epoch 912: Loss = 0.3531\n",
            "Epoch 913: Loss = 0.3530\n",
            "Epoch 914: Loss = 0.3531\n",
            "Epoch 915: Loss = 0.3531\n",
            "Epoch 916: Loss = 0.3531\n",
            "Epoch 917: Loss = 0.3530\n",
            "Epoch 918: Loss = 0.3529\n",
            "Epoch 919: Loss = 0.3529\n",
            "Epoch 920: Loss = 0.3529\n",
            "Epoch 921: Loss = 0.3529\n",
            "Epoch 922: Loss = 0.3528\n",
            "Epoch 923: Loss = 0.3527\n",
            "Epoch 924: Loss = 0.3527\n",
            "Epoch 925: Loss = 0.3527\n",
            "Epoch 926: Loss = 0.3527\n",
            "Epoch 927: Loss = 0.3527\n",
            "Epoch 928: Loss = 0.3526\n",
            "Epoch 929: Loss = 0.3526\n",
            "Epoch 930: Loss = 0.3526\n",
            "Epoch 931: Loss = 0.3526\n",
            "Epoch 932: Loss = 0.3525\n",
            "Epoch 933: Loss = 0.3525\n",
            "Epoch 934: Loss = 0.3525\n",
            "Epoch 935: Loss = 0.3525\n",
            "Epoch 936: Loss = 0.3524\n",
            "Epoch 937: Loss = 0.3524\n",
            "Epoch 938: Loss = 0.3524\n",
            "Epoch 939: Loss = 0.3524\n",
            "Epoch 940: Loss = 0.3523\n",
            "Epoch 941: Loss = 0.3523\n",
            "Epoch 942: Loss = 0.3523\n",
            "Epoch 943: Loss = 0.3522\n",
            "Epoch 944: Loss = 0.3522\n",
            "Epoch 945: Loss = 0.3522\n",
            "Epoch 946: Loss = 0.3522\n",
            "Epoch 947: Loss = 0.3522\n",
            "Epoch 948: Loss = 0.3521\n",
            "Epoch 949: Loss = 0.3521\n",
            "Epoch 950: Loss = 0.3521\n",
            "Epoch 951: Loss = 0.3521\n",
            "Epoch 952: Loss = 0.3520\n",
            "Epoch 953: Loss = 0.3520\n",
            "Epoch 954: Loss = 0.3520\n",
            "Epoch 955: Loss = 0.3520\n",
            "Epoch 956: Loss = 0.3519\n",
            "Epoch 957: Loss = 0.3519\n",
            "Epoch 958: Loss = 0.3519\n",
            "Epoch 959: Loss = 0.3519\n",
            "Epoch 960: Loss = 0.3518\n",
            "Epoch 961: Loss = 0.3518\n",
            "Epoch 962: Loss = 0.3518\n",
            "Epoch 963: Loss = 0.3518\n",
            "Epoch 964: Loss = 0.3518\n",
            "Epoch 965: Loss = 0.3517\n",
            "Epoch 966: Loss = 0.3517\n",
            "Epoch 967: Loss = 0.3517\n",
            "Epoch 968: Loss = 0.3517\n",
            "Epoch 969: Loss = 0.3516\n",
            "Epoch 970: Loss = 0.3516\n",
            "Epoch 971: Loss = 0.3516\n",
            "Epoch 972: Loss = 0.3516\n",
            "Epoch 973: Loss = 0.3516\n",
            "Epoch 974: Loss = 0.3515\n",
            "Epoch 975: Loss = 0.3515\n",
            "Epoch 976: Loss = 0.3515\n",
            "Epoch 977: Loss = 0.3515\n",
            "Epoch 978: Loss = 0.3514\n",
            "Epoch 979: Loss = 0.3514\n",
            "Epoch 980: Loss = 0.3514\n",
            "Epoch 981: Loss = 0.3514\n",
            "Epoch 982: Loss = 0.3514\n",
            "Epoch 983: Loss = 0.3513\n",
            "Epoch 984: Loss = 0.3513\n",
            "Epoch 985: Loss = 0.3513\n",
            "Epoch 986: Loss = 0.3513\n",
            "Epoch 987: Loss = 0.3513\n",
            "Epoch 988: Loss = 0.3512\n",
            "Epoch 989: Loss = 0.3512\n",
            "Epoch 990: Loss = 0.3512\n",
            "Epoch 991: Loss = 0.3512\n",
            "Epoch 992: Loss = 0.3511\n",
            "Epoch 993: Loss = 0.3511\n",
            "Epoch 994: Loss = 0.3511\n",
            "Epoch 995: Loss = 0.3511\n",
            "Epoch 996: Loss = 0.3511\n",
            "Epoch 997: Loss = 0.3510\n",
            "Epoch 998: Loss = 0.3510\n",
            "Epoch 999: Loss = 0.3510\n",
            "Epoch 1000: Loss = 0.3510\n",
            "Translated: \u0000\u0013\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, LayerNormalization, MultiHeadAttention\n",
        "import numpy as np\n",
        "\n",
        "# **데이터 준비**\n",
        "def tokenize_sentence(sentence, max_length=10):\n",
        "    # Create a vocabulary of unique characters\n",
        "    vocabulary = set(\" \".join(source_sentences + target_sentences))\n",
        "    # Create a mapping from character to index\n",
        "    char_to_index = {char: index for index, char in enumerate(vocabulary)}\n",
        "    # Tokenize the sentence using the character to index mapping\n",
        "    tokens = [char_to_index.get(char, 0) for char in sentence.ljust(max_length)]  # Use 0 for unknown characters\n",
        "    return np.array(tokens, dtype=np.int32)\n",
        "\n",
        "source_sentences = [\"I am a student\", \"Hello world\"]\n",
        "target_sentences = [\"Je suis étudiant\", \"Bonjour le monde\"]\n",
        "\n",
        "# Calculate the maximum length of all sentences\n",
        "max_length = max(len(s) for s in source_sentences + target_sentences)\n",
        "\n",
        "src_data = np.array([tokenize_sentence(s, max_length=max_length) for s in source_sentences])\n",
        "tgt_data = np.array([tokenize_sentence(s, max_length=max_length) for s in target_sentences])\n",
        "\n",
        "\n",
        "# **트랜스포머 레이어 정의**\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.attention(x, x)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# **트랜스포머 모델**\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embed_dim=16, num_heads=2, ff_dim=64):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.decoder = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.final_layer = Dense(vocab_size)\n",
        "\n",
        "    def call(self, src, tgt, training=False): # Added training argument with default False\n",
        "        src = self.embedding(src)\n",
        "        src = self.encoder(src, training=training) # Pass training to encoder\n",
        "\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.decoder(tgt, training=training) # Pass training to decoder\n",
        "\n",
        "        return self.final_layer(tgt)\n",
        "\n",
        "# **모델 초기화**\n",
        "vocab_size = 128  # ASCII 문자 개수\n",
        "model = Transformer(vocab_size)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# **모델 학습 (Training)**\n",
        "batch_size = 2\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(src_data, tgt_data[:, :-1])\n",
        "        loss = loss_fn(tgt_data[:, 1:], output)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "# **추론 함수 (Inference)**\n",
        "def translate(model, sentence, max_length=10):\n",
        "    src = tokenize_sentence(sentence, max_length).reshape(1, -1)\n",
        "    src_emb = model.embedding(src)\n",
        "    src_enc = model.encoder(src_emb, training=False) # Pass training=False to encoder\n",
        "\n",
        "    tgt_tokens = [tokenize_sentence(\"<sos>\", max_length)[0]]\n",
        "    for _ in range(max_length):\n",
        "        tgt = np.array(tgt_tokens).reshape(1, -1)\n",
        "        tgt_emb = model.embedding(tgt)\n",
        "        tgt_dec = model.decoder(tgt_emb, training=False) # Pass training=False to decoder\n",
        "        output = model.final_layer(tgt_dec)\n",
        "        next_token = np.argmax(output[:, -1, :])\n",
        "        if next_token == tokenize_sentence(\"<eos>\", max_length)[0]:\n",
        "            break\n",
        "        tgt_tokens.append(next_token)\n",
        "\n",
        "    return \"\".join([chr(i) for i in tgt_tokens])\n",
        "\n",
        "\n",
        "# **번역 실행**\n",
        "sentence = \"I am a student\"\n",
        "translation = translate(model, sentence)\n",
        "print(\"Translated:\", translation)\n"
      ]
    }
  ]
}