{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm6BZ8NcoXI6leOfRYOejp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/source_code/blob/main/SelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyfHZqsZgZh9",
        "outputId": "b27b5797-c17b-4eca-db02-276128c73ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Output: tf.Tensor(\n",
            "[[[ 0.9311143  -0.33381045  0.36613807 -0.69959617 -0.63235754\n",
            "    0.02820292  0.4311988  -0.24781424]\n",
            "  [-0.28204525  0.34441727  0.18293333 -0.06207423 -0.4488752\n",
            "   -0.94877803 -0.1723257   0.65572953]\n",
            "  [-0.4450016   0.30996057  0.08544388  0.02746993 -0.4649567\n",
            "   -1.0028745  -0.28625324  0.7357754 ]\n",
            "  [-0.19831698  0.13410234  0.25178906  0.05277795 -0.6322757\n",
            "   -0.793494   -0.03390846  0.6526605 ]\n",
            "  [-0.4789609   0.08307049 -0.00669652  0.14956263 -0.5012684\n",
            "   -0.7799514  -0.23635475  0.68860555]]\n",
            "\n",
            " [[ 0.96474886 -0.01032655 -0.13513587 -0.9784777   0.19818676\n",
            "    0.6600754  -1.1026492  -0.5322181 ]\n",
            "  [ 1.013519    0.1624079  -0.95928025 -1.4860604   0.5269492\n",
            "    0.57625806 -1.848207   -0.5674479 ]\n",
            "  [ 1.3793604  -0.19381861  0.3637418  -1.3125342   0.08677528\n",
            "    0.50825506 -0.82970864 -0.24490663]\n",
            "  [ 1.4154563  -0.09034033 -0.01997704 -1.5280592   0.2084867\n",
            "    0.5239569  -1.2390686  -0.31188726]\n",
            "  [ 0.28618154 -0.11962916 -0.39009106 -0.5582463   0.5146616\n",
            "    0.3094808  -0.43797576 -0.34972095]]], shape=(2, 5, 8), dtype=float32)\n",
            "Attention Weights: tf.Tensor(\n",
            "[[[0.11912909 0.04469449 0.20362192 0.5707628  0.0617916 ]\n",
            "  [0.31959727 0.15774283 0.04077123 0.02943394 0.45245472]\n",
            "  [0.2744815  0.23000087 0.0174395  0.03438259 0.44369552]\n",
            "  [0.10466015 0.21704721 0.22560409 0.18009435 0.27259412]\n",
            "  [0.14648056 0.33444795 0.09173851 0.16412434 0.26320878]]\n",
            "\n",
            " [[0.12356773 0.4381688  0.0414339  0.13738221 0.2594473 ]\n",
            "  [0.12447096 0.10057464 0.04803848 0.43590924 0.2910066 ]\n",
            "  [0.0888494  0.6744015  0.0477687  0.15503566 0.03394464]\n",
            "  [0.08684906 0.5294641  0.0136061  0.28341573 0.08666499]\n",
            "  [0.17991352 0.16624428 0.43047723 0.09531002 0.12805505]]], shape=(2, 5, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        queries = self.query_dense(inputs)\n",
        "        keys = self.key_dense(inputs)\n",
        "        values = self.value_dense(inputs)\n",
        "\n",
        "        attention_scores = tf.matmul(queries, keys, transpose_b=True)\n",
        "        attention_scores /= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        output = tf.matmul(attention_weights, values)\n",
        "        return output, attention_weights\n",
        "\n",
        "# 테스트 데이터 생성\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "d_model = 8\n",
        "\n",
        "inputs = tf.random.normal((batch_size, seq_length, d_model))\n",
        "self_attention = SelfAttention(d_model)\n",
        "output, attention_weights = self_attention(inputs)\n",
        "\n",
        "print(\"Self-Attention Output:\", output)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    }
  ]
}