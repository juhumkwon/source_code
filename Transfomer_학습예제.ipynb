{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqGAFa8XnDBrvHSkIs/C4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/source_code/blob/main/Transfomer_%ED%95%99%EC%8A%B5%EC%98%88%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYGD_60UH6SR",
        "outputId": "06d70525-5fb3-410b-c513-28d3fff6d2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.0445\n",
            "Epoch 10, Loss: 2.6871\n",
            "Epoch 20, Loss: 2.3337\n",
            "Epoch 30, Loss: 1.8612\n",
            "Epoch 40, Loss: 1.4572\n",
            "Epoch 50, Loss: 1.3457\n",
            "Epoch 60, Loss: 1.3257\n",
            "Epoch 70, Loss: 1.3214\n",
            "Epoch 80, Loss: 1.3192\n",
            "Epoch 90, Loss: 1.3179\n",
            "student. student. student. student. student.\n",
            "<unk> <unk> <unk> <unk> <unk>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. 데이터 준비\n",
        "raw_data = [\n",
        "    (\"나는 학생이다.\", \"I am a student.\"),\n",
        "    (\"그는 집에 갔다.\", \"He went home.\"),\n",
        "    (\"오늘 날씨가 좋다.\", \"The weather is nice today.\"),\n",
        "    (\"그녀는 책을 읽고 있다.\", \"She is reading a book.\"),\n",
        "    (\"우리는 학교에 간다.\", \"We go to school.\"),\n",
        "]\n",
        "\n",
        "input_texts, target_texts = zip(*raw_data)\n",
        "\n",
        "# 2. 토큰화\n",
        "tokenizer_ko = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
        "\n",
        "tokenizer_ko.fit_on_texts(input_texts)\n",
        "tokenizer_en.fit_on_texts(target_texts)\n",
        "\n",
        "input_sequences = tokenizer_ko.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer_en.texts_to_sequences(target_texts)\n",
        "\n",
        "max_len = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_len, padding='post')\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_size_ko = len(tokenizer_ko.word_index) + 1\n",
        "vocab_size_en = len(tokenizer_en.word_index) + 1\n",
        "\n",
        "# 3. Transformer 모델 정의\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size_ko, vocab_size_en, d_model=128, num_heads=4, dff=512):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding_ko = tf.keras.layers.Embedding(vocab_size_ko, d_model)\n",
        "        self.embedding_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
        "        self.encoder_layer = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=d_model)\n",
        "        self.decoder_layer = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size_en)\n",
        "\n",
        "    def call(self, encoder_input, decoder_input):\n",
        "        enc_embed = self.embedding_ko(encoder_input)\n",
        "        enc_output = self.encoder_layer(enc_embed, enc_embed, enc_embed)\n",
        "        dec_embed = self.embedding_en(decoder_input)\n",
        "        dec_output = self.decoder_layer(dec_embed, enc_output, enc_output)\n",
        "        final_output = self.ffn(dec_output)\n",
        "        return self.final_layer(final_output)\n",
        "\n",
        "# 4. 모델 생성 및 학습 설정\n",
        "transformer = Transformer(vocab_size_ko, vocab_size_en)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_sequences = np.array(target_sequences)\n",
        "\n",
        "# 5. 학습 실행\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(input_sequences, target_sequences[:, :-1])\n",
        "        loss = loss_fn(target_sequences[:, 1:], predictions)\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "# 6. 번역 함수\n",
        "def translate(input_sentence):\n",
        "    input_seq = tokenizer_ko.texts_to_sequences([input_sentence])\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "    decoder_input = np.zeros((1, max_len))\n",
        "    for i in range(max_len):\n",
        "        predictions = transformer(input_seq, decoder_input)\n",
        "        predicted_id = np.argmax(predictions[0, i])\n",
        "        decoder_input[0, i] = predicted_id\n",
        "        if predicted_id == 0:\n",
        "            break\n",
        "    translated_tokens = tokenizer_en.sequences_to_texts(decoder_input)\n",
        "    return translated_tokens[0]\n",
        "\n",
        "# 7. 테스트 실행\n",
        "print(translate(\"나는 학생이다.\"))\n",
        "print(translate(\"그는 집에 갔다.\"))"
      ]
    }
  ]
}